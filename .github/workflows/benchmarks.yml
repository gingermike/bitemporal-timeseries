name: Release Performance Benchmarks

on:
  push:
    tags:
      - 'v*'  # Trigger on version tags like v1.0.0
  # Allow manual trigger for testing
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

# Note: This workflow complements build-wheels.yml for releases
# - benchmarks.yml: Runs on version tags, generates performance reports for GitHub Pages
# - build-wheels.yml: Runs on version tags, builds and publishes Python wheels
# Both workflows run simultaneously on version tags to create complete releases

jobs:
  benchmark:
    name: Run Benchmarks and Generate Flamegraphs
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Extract version from tag
      id: version
      run: |
        if [[ $GITHUB_REF == refs/tags/* ]]; then
          VERSION=${GITHUB_REF#refs/tags/v}
          echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
          echo "FULL_TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
        else
          echo "VERSION=dev" >> $GITHUB_OUTPUT
          echo "FULL_TAG=dev" >> $GITHUB_OUTPUT
        fi
        echo "Building benchmarks for version: $VERSION"
      
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable
        components: rustfmt, clippy

    - name: Cache cargo registry
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Install Python dependencies
      run: |
        python3 -m pip install --upgrade pip

    - name: Build project
      run: cargo build --release

    - name: Run benchmarks and generate flamegraphs
      run: |
        echo "🔥 Running core benchmarks with flamegraph generation..."
        
        # Generate flamegraphs for key performance analysis benchmarks (3-5 seconds each)
        cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 3
        cargo bench --bench bitemporal_benchmarks conflation_effectiveness -- --profile-time 3
        
        # Large dataset benchmark with shorter profiling to avoid timeout
        timeout 300s cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500000" -- --profile-time 2 || echo "⚠️ Large benchmark timed out - continuing"
        
        echo "📊 Running additional benchmarks for completeness..."
        
        # Run smaller benchmarks without flamegraphs for speed
        cargo bench --bench bitemporal_benchmarks small_dataset
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/100"
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500"
        
        # Run parallel effectiveness tests
        cargo bench --bench bitemporal_benchmarks "parallel_effectiveness/scenario/balanced_workload"
        
        echo "✅ All benchmarks completed"
        
    - name: Add flamegraph links to HTML reports
      run: |
        echo "🔗 Adding flamegraph links to HTML reports..."
        python3 scripts/add_flamegraphs_to_html.py
        echo "✅ HTML reports updated with flamegraph links"

    - name: Prepare GitHub Pages content
      run: |
        echo "📦 Preparing GitHub Pages content..."
        
        # Copy criterion reports to a pages directory
        mkdir -p pages
        cp -r target/criterion/* pages/
        
        # Create a landing page with version information
        cat > pages/index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>Bitemporal Timeseries v${{ steps.version.outputs.VERSION }} - Performance Benchmarks</title>
            <meta charset="utf-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
                .container { max-width: 1200px; margin: 0 auto; }
                h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
                h2 { color: #34495e; margin-top: 30px; }
                .benchmark-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
                .benchmark-card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; background: #f9f9f9; }
                .benchmark-card h3 { margin-top: 0; color: #2980b9; }
                .benchmark-links { margin: 10px 0; }
                .benchmark-links a { display: inline-block; margin: 5px 10px 5px 0; padding: 8px 12px; background: #3498db; color: white; text-decoration: none; border-radius: 4px; font-size: 14px; }
                .benchmark-links a:hover { background: #2980b9; }
                .flamegraph-link { background: #e74c3c !important; }
                .flamegraph-link:hover { background: #c0392b !important; }
                .summary { background: #ecf0f1; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .footer { margin-top: 40px; padding-top: 20px; border-top: 1px solid #bdc3c7; text-align: center; color: #7f8c8d; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>🚀 Bitemporal Timeseries v${{ steps.version.outputs.VERSION }} Performance Benchmarks</h1>
                
                <div class="summary">
                    <h2>📊 Performance Summary</h2>
                    <p><strong>Release:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>This page contains comprehensive performance benchmarks for the bitemporal timeseries processing algorithm, including flamegraphs for detailed performance analysis.</p>
                    <ul>
                        <li><strong>Small Dataset (5 records)</strong>: ~30-35 µs</li>
                        <li><strong>Medium Dataset (100 records)</strong>: ~165-170 µs</li> 
                        <li><strong>Large Dataset (500k records)</strong>: ~900-950 ms</li>
                        <li><strong>Conflation Effectiveness</strong>: ~28 µs</li>
                    </ul>
                </div>

                <h2>🎯 Core Benchmarks</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Small Dataset</h3>
                        <p>Basic functionality test with 5 records</p>
                        <div class="benchmark-links">
                            <a href="small_dataset/report/">📈 Report</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Medium Dataset</h3>
                        <p>Medium-scale test with 100 current records and 20 updates</p>
                        <div class="benchmark-links">
                            <a href="medium_dataset/report/">📈 Report</a>
                            <a href="medium_dataset/profile/flamegraph.svg" class="flamegraph-link">🔥 Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Conflation Effectiveness</h3>
                        <p>Tests the effectiveness of adjacent segment merging</p>
                        <div class="benchmark-links">
                            <a href="conflation_effectiveness/report/">📈 Report</a>
                            <a href="conflation_effectiveness/profile/flamegraph.svg" class="flamegraph-link">🔥 Flamegraph</a>
                        </div>
                    </div>
                </div>

                <h2>📏 Scaling Analysis</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Dataset Size Scaling</h3>
                        <p>Performance analysis across different dataset sizes</p>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/report/">📈 Overview</a>
                            <a href="scaling_by_dataset_size/records/10/report/">10 records</a>
                            <a href="scaling_by_dataset_size/records/50/report/">50 records</a>
                            <a href="scaling_by_dataset_size/records/100/report/">100 records</a>
                            <a href="scaling_by_dataset_size/records/500/report/">500 records</a>
                            <a href="scaling_by_dataset_size/records/500000/report/">500k records</a>
                        </div>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/records/500000/profile/flamegraph.svg" class="flamegraph-link">🔥 Large Dataset Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Parallel Effectiveness</h3>
                        <p>Analysis of parallelization effectiveness across different workload patterns</p>
                        <div class="benchmark-links">
                            <a href="parallel_effectiveness/report/">📈 Overview</a>
                            <a href="parallel_effectiveness/scenario/few_ids_many_records/report/">Few IDs, Many Records</a>
                            <a href="parallel_effectiveness/scenario/many_ids_few_records/report/">Many IDs, Few Records</a>
                            <a href="parallel_effectiveness/scenario/balanced_workload/report/">Balanced Workload</a>
                        </div>
                    </div>
                </div>

                <h2>🔥 Understanding Flamegraphs</h2>
                <div class="summary">
                    <p><strong>Flamegraphs</strong> show exactly where your code spends time:</p>
                    <ul>
                        <li><strong>Width</strong>: How much time is spent in each function</li>
                        <li><strong>Height</strong>: Call stack depth</li>
                        <li><strong>Colors</strong>: Different functions/modules</li>
                        <li><strong>Click</strong>: Zoom into specific functions</li>
                    </ul>
                    <p>Look for wide bars to identify performance hotspots in the bitemporal algorithm.</p>
                </div>

                <div class="footer">
                    <p>Generated by <a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> with flamegraph integration via <a href="https://github.com/tikv/pprof-rs">pprof-rs</a></p>
                    <p><strong>Version:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>View other releases: <a href="https://github.com/your-username/bitemporal-timeseries/releases">Release History</a></p>
                </div>
            </div>
        </body>
        </html>
        EOF
        
        echo "✅ GitHub Pages content prepared"

    - name: Upload GitHub Pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./pages

  deploy:
    name: Deploy to GitHub Pages
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: benchmark
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4